{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3554197b",
   "metadata": {},
   "source": [
    "# Data Preperation for modelling in Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5244c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb279f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "df = pd.read_parquet(\"../../data/processed/data_combined.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03feff9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[\"actual_load_mw\"].to_frame()\n",
    "X = df.drop(columns=[\"actual_load_mw\"])\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.33, random_state=12345)\n",
    "\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aebe0b7",
   "metadata": {},
   "source": [
    "## Intermediate Outlier check\n",
    "Checking for outliers is important to help choose the most appropriate scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a985842d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tp            0.107470\n",
       "ssrd          0.047656\n",
       "wind_speed    0.025069\n",
       "t2m           0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def iqr_outlier_stats(X):\n",
    "    stats = {}\n",
    "    for col in X.columns:\n",
    "        q1 = X[col].quantile(0.25)\n",
    "        q3 = X[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        if iqr == 0:\n",
    "            stats[col] = 0.0\n",
    "            continue\n",
    "        mask = (X[col] < q1 - 1.5 * iqr) | (X[col] > q3 + 1.5 * iqr)\n",
    "        stats[col] = mask.mean()\n",
    "    return pd.Series(stats).sort_values(ascending=False)\n",
    "\n",
    "outlier_frac = iqr_outlier_stats(X_train)\n",
    "outlier_frac\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fb2db89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actual_load_mw    0.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outlier_frac = iqr_outlier_stats(y_train)\n",
    "outlier_frac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbbeb86a",
   "metadata": {},
   "source": [
    "Seems like the total precipitation has a 10% outlier rate. This would suggest using a robust scaler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f015492",
   "metadata": {},
   "source": [
    "## Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c508ecde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled shapes - Train: (164828, 4), Val: (40592, 4), Test: (40592, 4)\n"
     ]
    }
   ],
   "source": [
    "X_scaler = RobustScaler()\n",
    "y_scaler = RobustScaler()\n",
    "\n",
    "# Fit scalers on training data only\n",
    "X_train_scaled = X_scaler.fit_transform(X_train)\n",
    "y_train_scaled = y_scaler.fit_transform(y_train).squeeze()\n",
    "\n",
    "# Transform validation and test sets using fitted scalers\n",
    "X_val_scaled = X_scaler.transform(X_val)\n",
    "X_test_scaled = X_scaler.transform(X_test)\n",
    "\n",
    "y_val_scaled = y_scaler.transform(y_val).squeeze()\n",
    "y_test_scaled = y_scaler.transform(y_test).squeeze()\n",
    "\n",
    "print(f\"Scaled shapes - Train: {X_train_scaled.shape}, Val: {X_val_scaled.shape}, Test: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbe938b",
   "metadata": {},
   "source": [
    "## Windowing\n",
    "\n",
    "Create sequences for time series forecasting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b752b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sequences: (164061, 672, 4) -> (164061, 96)\n",
      "Val sequences:   (39825, 672, 4) -> (39825, 96)\n",
      "Test sequences:  (39825, 672, 4) -> (39825, 96)\n"
     ]
    }
   ],
   "source": [
    "def create_sequences(X, y, window_size=4*24*7, forecast_horizon=4*24):\n",
    "    \"\"\"\n",
    "    Create sequences for time series forecasting.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature array (n_samples, n_features)\n",
    "        y: Target array (n_samples,)\n",
    "        window_size: Number of time steps to look back\n",
    "        forecast_horizon: Number of time steps to forecast ahead\n",
    "        \n",
    "    Returns:\n",
    "        X_seq: Array of shape (n_sequences, window_size, n_features)\n",
    "        y_seq: Array of shape (n_sequences, forecast_horizon)\n",
    "    \"\"\"\n",
    "    n_samples = len(X)\n",
    "    \n",
    "    X_seq = []\n",
    "    y_seq = []\n",
    "    \n",
    "    for i in range(n_samples - window_size - forecast_horizon + 1):\n",
    "        X_seq.append(X[i:i+window_size])\n",
    "        y_seq.append(y[i+window_size:i+window_size+forecast_horizon])\n",
    "    \n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Configuration\n",
    "WINDOW_SIZE = 4*24*7  # Look back the last 7 days\n",
    "FORECAST_HORIZON = 4*24  # Forecast 96 steps ahead\n",
    "\n",
    "# Create sequences for each split\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, WINDOW_SIZE, FORECAST_HORIZON)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, WINDOW_SIZE, FORECAST_HORIZON)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, WINDOW_SIZE, FORECAST_HORIZON)\n",
    "\n",
    "print(f\"Train sequences: {X_train_seq.shape} -> {y_train_seq.shape}\")\n",
    "print(f\"Val sequences:   {X_val_seq.shape} -> {y_val_seq.shape}\")\n",
    "print(f\"Test sequences:  {X_test_seq.shape} -> {y_test_seq.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ad6de4",
   "metadata": {},
   "source": [
    "## Save Data and Artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a499fbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data to ..\\..\\data\\processed\\ml_data\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "output_dir = Path(\"../../data/processed/ml_data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "scalers_dir = output_dir / \"scalers\"\n",
    "scalers_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save numpy arrays\n",
    "np.save(output_dir / \"X_train.npy\", X_train_seq)\n",
    "np.save(output_dir / \"y_train.npy\", y_train_seq)\n",
    "np.save(output_dir / \"X_val.npy\", X_val_seq)\n",
    "np.save(output_dir / \"y_val.npy\", y_val_seq)\n",
    "np.save(output_dir / \"X_test.npy\", X_test_seq)\n",
    "np.save(output_dir / \"y_test.npy\", y_test_seq)\n",
    "\n",
    "# Save scalers\n",
    "with open(scalers_dir / \"X_scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(X_scaler, f)\n",
    "with open(scalers_dir / \"y_scaler.pkl\", \"wb\") as f:\n",
    "    pickle.dump(y_scaler, f)\n",
    "\n",
    "# Save metadata\n",
    "meta = {\n",
    "    \"window_size\": WINDOW_SIZE,\n",
    "    \"forecast_horizon\": FORECAST_HORIZON,\n",
    "    \"n_features\": X_train_scaled.shape[1],\n",
    "    \"feature_names\": list(X.columns),\n",
    "    \"train_samples\": len(X_train_seq),\n",
    "    \"val_samples\": len(X_val_seq),\n",
    "    \"test_samples\": len(X_test_seq),\n",
    "    \"train_date_range\": {\n",
    "        \"start\": str(X_train.index.min()),\n",
    "        \"end\": str(X_train.index.max())\n",
    "    },\n",
    "    \"val_date_range\": {\n",
    "        \"start\": str(X_val.index.min()),\n",
    "        \"end\": str(X_val.index.max())\n",
    "    },\n",
    "    \"test_date_range\": {\n",
    "        \"start\": str(X_test.index.min()),\n",
    "        \"end\": str(X_test.index.max())\n",
    "    },\n",
    "    \"scaler_type\": \"RobustScaler\",\n",
    "    \"split_ratio\": {\n",
    "        \"train\": 0.7,\n",
    "        \"val\": 0.15,\n",
    "        \"test\": 0.15\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(output_dir / \"meta.json\", \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(f\"Saved data to {output_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
