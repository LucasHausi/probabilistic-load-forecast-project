{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b8196e2",
   "metadata": {},
   "source": [
    "# Bayesian LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeaf072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import mlflow\n",
    "\n",
    "import blitz.modules as blitz_modules\n",
    "from blitz.utils import variational_estimator\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "\n",
    "import copy\n",
    "import optuna\n",
    "\n",
    "import json\n",
    "from mlflow.models.signature import ModelSignature\n",
    "from mlflow.types import Schema, TensorSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7610343",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "DATA_PATH = Path(\"../../data/processed\")\n",
    "\n",
    "EXPERIMENT_NAME = \"fs_06_load_calendar_future_weather\"\n",
    "OUTPUT_PATH = DATA_PATH / \"ml_data\" / EXPERIMENT_NAME\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df9e83c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cudnn.enabled       = True\n",
      "cudnn.benchmark     = False\n",
      "cudnn.deterministic = True\n"
     ]
    }
   ],
   "source": [
    "# Possible fix for stability in searches\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(\"cudnn.enabled       =\", torch.backends.cudnn.enabled)\n",
    "print(\"cudnn.benchmark     =\", torch.backends.cudnn.benchmark)\n",
    "print(\"cudnn.deterministic =\", torch.backends.cudnn.deterministic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148ffa52",
   "metadata": {},
   "source": [
    "## First: Start the MLFlow-Server for logging the experiments\n",
    "\n",
    "Run the following command: \n",
    "\n",
    "Powershell:\n",
    "\n",
    "mlflow server ` \n",
    "    --backend-store-uri sqlite:///mlflow.db ` \n",
    "    --default-artifact-root ./mlartifacts/  \n",
    "\n",
    "\n",
    "Bash / Git Bash / WSL / Linux / macOS\n",
    "mlflow server \\\n",
    "  --backend-store-uri sqlite:///mlflow.db \\\n",
    "  --default-artifact-root ./mlartifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9fcd4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")\n",
    "mlflow.set_experiment(\"load_forecasting_bayesian_lstm\")\n",
    "\n",
    "mlflow.enable_system_metrics_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186b6e68",
   "metadata": {},
   "source": [
    "## Defining the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a2bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "@variational_estimator\n",
    "class BayesianLSTM(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, horizon):\n",
    "        super().__init__()\n",
    "        self.lstm = blitz_modules.BayesianLSTM(\n",
    "            in_features=input_size,\n",
    "            out_features=hidden_size\n",
    "        )\n",
    "\n",
    "        self.fc = torch.nn.Linear(hidden_size, 2 * horizon) # times two, because we need to output a mean and variance for the GausianNLLLoss\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # OLD (incorrect for BLiTZ BayesianLSTM):\n",
    "        # x is in format (batch_size, seq_len, input_size) but (seq_len, batch_size, input_size) is required\n",
    "        # x = x.transpose(0, 1) # now in the correct format\n",
    "        # out, _ = self.lstm(x) # (seq_len, batch_size, input_size)\n",
    "        # h = out[-1, :, :] # The last hidden state (batch_size, hidden_size)\n",
    "        # return self.fc(h) # (batch, 2 * horizon) each batch having the shape [mu_1, mu_2, ... var_1, var_2 ...]\n",
    "\n",
    "        # Correct: BLiTZ BayesianLSTM expects (batch_size, seq_len, input_size)\n",
    "        out, _ = self.lstm(x)\n",
    "        h = out[:, -1, :]  # last time step for each sample\n",
    "        return self.fc(h)  # (batch, 2 * horizon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45aa856",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e7edd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scaler\n",
    "scalers_dir = OUTPUT_PATH / \"scalers\"\n",
    "\n",
    "with open(scalers_dir / \"y_scaler.pkl\", \"rb\") as f:\n",
    "    y_scaler = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea49404",
   "metadata": {},
   "source": [
    "## Define the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b53b204a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 164214 samples\n",
      "Val dataset:   39863 samples\n",
      "Test dataset:  39863 samples\n",
      "Sample shape: X=torch.Size([672, 7]), y=torch.Size([96])\n"
     ]
    }
   ],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for time series forecasting.\"\"\"\n",
    "    \n",
    "    def __init__(self, X, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: Input sequences (n_samples, window_size, n_features)\n",
    "            y: Target values (n_samples, forecast_horizon)\n",
    "        \"\"\"\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.y = torch.from_numpy(y).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "X_train_seq = np.load(OUTPUT_PATH / \"X_train.npy\")\n",
    "y_train_seq = np.load(OUTPUT_PATH / \"y_train.npy\")\n",
    "train_dataset = TimeSeriesDataset(X_train_seq, y_train_seq)\n",
    "# del X_train_seq, y_train_seq\n",
    "gc.collect()\n",
    "\n",
    "X_val_seq = np.load(OUTPUT_PATH / \"X_val.npy\")\n",
    "y_val_seq = np.load(OUTPUT_PATH / \"y_val.npy\")\n",
    "val_dataset = TimeSeriesDataset(X_val_seq, y_val_seq)\n",
    "del X_val_seq, y_val_seq\n",
    "gc.collect()\n",
    "\n",
    "X_test_seq = np.load(OUTPUT_PATH / \"X_test.npy\")\n",
    "y_test_seq = np.load(OUTPUT_PATH / \"y_test.npy\")\n",
    "test_dataset = TimeSeriesDataset(X_test_seq, y_test_seq)\n",
    "del X_test_seq, y_test_seq\n",
    "gc.collect()\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} samples\")\n",
    "print(f\"Val dataset:   {len(val_dataset)} samples\")\n",
    "print(f\"Test dataset:  {len(test_dataset)} samples\")\n",
    "print(f\"Sample shape: X={train_dataset[0][0].shape}, y={train_dataset[0][1].shape}\")\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b51067",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e4f75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    with mlflow.start_run(run_name=f\"trial_{trial.number}\", nested=True):\n",
    "        hidden_size = trial.suggest_int(\"hidden_size\", 64, 256, step=32)\n",
    "        lr = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "        weight_decay = trial.suggest_float(\"weight_decay\", 1e-7, 1e-3, log=True)\n",
    "        sample_nbr = trial.suggest_categorical(\"sample_nbr\", [3, 5, 7, 10])\n",
    "        eval_sample_nbr = 50\n",
    "        kl_warmup_epochs = trial.suggest_int(\"kl_warmup_epochs\", 5, 25)\n",
    "\n",
    "        mlflow.log_params(trial.params)\n",
    "        mlflow.log_param(\"eval_sample_nbr\", eval_sample_nbr)\n",
    "\n",
    "        model = BayesianLSTM(\n",
    "            input_size=train_dataset[0][0].shape[-1],\n",
    "            hidden_size=hidden_size,\n",
    "            horizon=train_dataset[0][1].shape[-1],\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "\n",
    "        gauss_nll = torch.nn.GaussianNLLLoss(eps=1e-6, reduction=\"mean\") #torch.nn.MSELoss()\n",
    "\n",
    "        def nll_criterion(pred_params, y):\n",
    "            mu, log_var = torch.chunk(pred_params, 2, dim=-1)\n",
    "            var = torch.nn.functional.softplus(log_var) + 1e-6\n",
    "            return gauss_nll(mu, y, var)\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        val_dataloader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        base_kl_weight = 1.0 / len(train_dataset)\n",
    "        best_val = float(\"inf\")\n",
    "        best_state = None\n",
    "        best_epoch = -1\n",
    "        patience = 10\n",
    "        wait = 0\n",
    "        num_epochs = 50\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            train_elbo = 0.0\n",
    "\n",
    "            kl_weight = base_kl_weight * min(1.0, (epoch + 1) / kl_warmup_epochs)\n",
    "\n",
    "            for xb, yb in train_dataloader:\n",
    "                xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                elbo = model.sample_elbo(\n",
    "                    inputs=xb,\n",
    "                    labels=yb,\n",
    "                    criterion=nll_criterion,\n",
    "                    sample_nbr=sample_nbr,\n",
    "                    complexity_cost_weight=kl_weight,\n",
    "                )\n",
    "\n",
    "                elbo.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "\n",
    "                train_elbo += elbo.item()\n",
    "\n",
    "            train_elbo /= len(train_dataloader)\n",
    "            mlflow.log_metric(\"train_elbo\", train_elbo, step=epoch)\n",
    "\n",
    "            model.eval()\n",
    "            val_elbo = 0.0\n",
    "            val_nll = 0.0\n",
    "            val_mu_preds = []\n",
    "            val_targets = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_dataloader:\n",
    "                    xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "                    elbo = model.sample_elbo(\n",
    "                        inputs=xb,\n",
    "                        labels=yb,\n",
    "                        criterion=nll_criterion,\n",
    "                        sample_nbr=sample_nbr,\n",
    "                        complexity_cost_weight=kl_weight,\n",
    "                    )\n",
    "\n",
    "                    pred_samples = torch.stack([model(xb) for _ in range(eval_sample_nbr)], dim=0)\n",
    "                    mu_s, log_var_s = torch.chunk(pred_samples, 2, dim=-1)\n",
    "                    var_s = torch.nn.functional.softplus(log_var_s) + 1e-6\n",
    "                    mu_pred = mu_s.mean(dim=0)\n",
    "                    var_pred = var_s.mean(dim=0) + mu_s.var(dim=0, unbiased=False)\n",
    "                    nll = gauss_nll(mu_pred, yb, var_pred)\n",
    "\n",
    "                    val_mu_preds.append(mu_pred.detach().cpu().numpy())\n",
    "                    val_targets.append(yb.detach().cpu().numpy())\n",
    "\n",
    "                    val_elbo += elbo.item()\n",
    "                    val_nll += nll.item()\n",
    "\n",
    "            val_elbo /= len(val_dataloader)\n",
    "            val_nll /= len(val_dataloader)\n",
    "            y_pred_val = np.concatenate(val_mu_preds, axis=0)\n",
    "            y_true_val = np.concatenate(val_targets, axis=0)\n",
    "            val_mae = mean_absolute_error(y_true_val, y_pred_val)\n",
    "            val_rmse = root_mean_squared_error(y_true_val, y_pred_val)\n",
    "            val_mse = val_rmse ** 2\n",
    "            val_rmse_h = [\n",
    "                root_mean_squared_error(y_true_val[:, h], y_pred_val[:, h])\n",
    "                for h in [0, 3, 11, 23, 47, 95]\n",
    "            ]\n",
    "\n",
    "            mlflow.log_metric(\"val_elbo\", val_elbo, step=epoch)\n",
    "            mlflow.log_metric(\"val_nll\", val_nll, step=epoch)\n",
    "            mlflow.log_metric(\"val_mse\", val_mse, step=epoch)\n",
    "            mlflow.log_metric(\"val_mae\", val_mae, step=epoch)\n",
    "            mlflow.log_metric(\"val_rmse\", val_rmse, step=epoch)\n",
    "            for h_idx, rmse_h in enumerate(val_rmse_h):\n",
    "                mlflow.log_metric(f\"val_rmse_h{h_idx+1}\", rmse_h, step=epoch)\n",
    "            mlflow.log_metric(\"kl_weight\", kl_weight, step=epoch)\n",
    "\n",
    "            trial.report(val_nll, epoch)\n",
    "\n",
    "            if trial.should_prune():\n",
    "                del model, optimizer\n",
    "                gc.collect()\n",
    "                if torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "            if val_nll < best_val:\n",
    "                best_val = val_nll\n",
    "                wait = 0\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "                best_epoch = epoch\n",
    "            else:\n",
    "                wait += 1\n",
    "\n",
    "            if wait >= patience:\n",
    "                break\n",
    "\n",
    "            print(\n",
    "                \n",
    "                f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "                f\"Train ELBO: {train_elbo:.4f}, \"\n",
    "                f\"Val ELBO: {val_elbo:.4f}, \"\n",
    "                f\"Val NLL: {val_nll:.6f}, \"\n",
    "                f\"Val MSE: {val_mse:.6f}, \"\n",
    "                f\"Val MAE: {val_mae:.6f}, \"\n",
    "                f\"KL w: {kl_weight:.2e}\"\n",
    "            )\n",
    "\n",
    "        if best_state is not None:\n",
    "            model.load_state_dict(best_state)\n",
    "            mlflow.log_metric(\"best_epoch\", best_epoch)\n",
    "            mlflow.pytorch.log_model(model, artifact_path=\"best_model\")\n",
    "\n",
    "        mlflow.log_metric(\"best_val_nll\", best_val)\n",
    "\n",
    "        del model, optimizer\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return best_val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c9a94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "STUDY_NAME = f\"bayesian_lstm_nll_{EXPERIMENT_NAME}\"\n",
    "STUDY_STORAGE = \"sqlite:///optuna_bayesian_lstm.db\"\n",
    "\n",
    "study = optuna.create_study(\n",
    "    study_name=STUDY_NAME,\n",
    "    storage=STUDY_STORAGE,\n",
    "    load_if_exists=True,\n",
    "    direction=\"minimize\",\n",
    "    sampler=optuna.samplers.TPESampler(seed=1234),\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=10),\n",
    ")\n",
    "\n",
    "with mlflow.start_run(run_name=f\"optuna_search_bayesian_lstm_nll_{EXPERIMENT_NAME}\"):\n",
    "    study.optimize(objective, n_trials=50, show_progress_bar=True, gc_after_trial=True)\n",
    "    mlflow.log_metric(\"best_val_nll\", study.best_value)\n",
    "    mlflow.log_params(study.best_params)\n",
    "    mlflow.log_param(\"best_trial_number\", study.best_trial.number)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd3802a",
   "metadata": {},
   "source": [
    "## Final training after the hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f333951",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_path = OUTPUT_PATH / \"meta.json\"\n",
    "with open(meta_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "seq_len = meta[\"window_size\"]          # 672\n",
    "n_features = meta[\"n_features\"]        # num_feautes\n",
    "horizon = meta[\"forecast_horizon\"]     # 96\n",
    "\n",
    "assert X_train_seq.shape[1:] == (seq_len, n_features)\n",
    "assert y_train_seq.shape[1] == horizon\n",
    "\n",
    "input_example = X_train_seq[:2].astype(np.float32)\n",
    "\n",
    "signature = ModelSignature(\n",
    "    inputs=Schema([\n",
    "        TensorSpec(np.dtype(np.float32), (-1, seq_len, n_features), name=\"x\")\n",
    "    ]),\n",
    "    outputs=Schema([\n",
    "        TensorSpec(np.dtype(np.float32), (-1, horizon), name=\"yhat\")\n",
    "    ]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46b2683e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/11 15:29:31 INFO mlflow.system_metrics.system_metrics_monitor: Skip logging GPU metrics. Set logger level to DEBUG for more details.\n",
      "2026/02/11 15:29:31 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60, Train ELBO: -0.8950, Val ELBO: -1.0356, Val NLL: -1.078038, KL w: 2.03e-07\n",
      "Epoch 4/60, Train ELBO: -1.2710, Val ELBO: -1.1639, Val NLL: -1.273162, KL w: 8.12e-07\n",
      "Epoch 7/60, Train ELBO: -1.3009, Val ELBO: -1.1345, Val NLL: -1.293617, KL w: 1.42e-06\n",
      "Epoch 10/60, Train ELBO: -1.3518, Val ELBO: -1.1672, Val NLL: -1.369472, KL w: 2.03e-06\n",
      "Epoch 13/60, Train ELBO: -1.3183, Val ELBO: -1.1042, Val NLL: -1.379954, KL w: 2.64e-06\n",
      "Epoch 16/60, Train ELBO: -1.2767, Val ELBO: -1.1076, Val NLL: -1.423049, KL w: 3.25e-06\n",
      "Epoch 19/60, Train ELBO: -1.2287, Val ELBO: -1.0540, Val NLL: -1.439635, KL w: 3.86e-06\n",
      "Epoch 22/60, Train ELBO: -1.1782, Val ELBO: -0.9934, Val NLL: -1.450022, KL w: 4.47e-06\n",
      "Epoch 25/60, Train ELBO: -1.1302, Val ELBO: -0.9691, Val NLL: -1.475837, KL w: 5.07e-06\n",
      "Epoch 28/60, Train ELBO: -1.0850, Val ELBO: -0.9302, Val NLL: -1.479073, KL w: 5.68e-06\n",
      "Epoch 31/60, Train ELBO: -1.0577, Val ELBO: -0.8847, Val NLL: -1.475613, KL w: 6.09e-06\n",
      "Epoch 34/60, Train ELBO: -1.0612, Val ELBO: -0.8607, Val NLL: -1.438631, KL w: 6.09e-06\n",
      "Epoch 37/60, Train ELBO: -1.0633, Val ELBO: -0.8854, Val NLL: -1.473546, KL w: 6.09e-06\n",
      "Epoch 40/60, Train ELBO: -1.0640, Val ELBO: -0.8711, Val NLL: -1.466834, KL w: 6.09e-06\n",
      "Epoch 43/60, Train ELBO: -1.0643, Val ELBO: -0.8880, Val NLL: -1.478178, KL w: 6.09e-06\n",
      "Epoch 46/60, Train ELBO: -1.0649, Val ELBO: -0.8659, Val NLL: -1.464870, KL w: 6.09e-06\n",
      "Epoch 49/60, Train ELBO: -1.0651, Val ELBO: -0.8826, Val NLL: -1.479626, KL w: 6.09e-06\n",
      "Epoch 52/60, Train ELBO: -1.0678, Val ELBO: -0.8702, Val NLL: -1.468140, KL w: 6.09e-06\n",
      "Epoch 55/60, Train ELBO: -1.0689, Val ELBO: -0.8589, Val NLL: -1.459909, KL w: 6.09e-06\n",
      "Epoch 58/60, Train ELBO: -1.0709, Val ELBO: -0.8723, Val NLL: -1.463839, KL w: 6.09e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/14 09:27:10 WARNING mlflow.utils.requirements_utils: Found torch version (2.9.1+cu128) contains a local version label (+cu128). MLflow logged a pip requirement for this package as 'torch==2.9.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2026/02/14 09:27:25 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e8b28baa164b899713ea2b686a515f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/14 09:27:25 WARNING mlflow.models.model: Failed to validate serving input example {\n",
      "  \"inputs\": [\n",
      "    [\n",
      "      [\n",
      "        -0.879765391.... Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\n",
      "Got error: The PyTorch flavor does not support List or Dict input types. Please use a pandas.DataFrame or a numpy.ndarray\n",
      "2026/02/14 09:27:25 INFO mlflow.models.model: Found the following environment variables used during model inference: [CDSAPI_KEY]. Please check if you need to set them when deploying the model. To disable this message, set environment variable `MLFLOW_RECORD_ENV_VARS_IN_MODEL_LOGGING` to `false`.\n",
      "2026/02/14 11:23:29 INFO mlflow.system_metrics.system_metrics_monitor: Stopping system metrics monitoring...\n",
      "2026/02/14 11:23:29 INFO mlflow.system_metrics.system_metrics_monitor: Successfully terminated system metrics monitoring!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run baysian_lstm_nll_fs_06_load_calendar_future_weather at: http://127.0.0.1:5000/#/experiments/3/runs/2ccd3a12be4d4ec484be5af5fc8ab4e3\n",
      "ðŸ§ª View experiment at: http://127.0.0.1:5000/#/experiments/3\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 60\n",
    "HIDDEN_SIZE = 64\n",
    "SAMPLE_NBR = 3\n",
    "VAL_EVAL_SAMPLE_NBR = 10\n",
    "TEST_SAMPLE_NBR = 70\n",
    "KL_WARMUP_EPOCHS = 30\n",
    "\n",
    "optim_kwargs = {\n",
    "    \"lr\": 3e-4,\n",
    "    \"weight_decay\": 1e-5,\n",
    "}\n",
    "\n",
    "model = BayesianLSTM(\n",
    "    input_size=train_dataset[0][0].shape[-1],\n",
    "    horizon=train_dataset[0][1].shape[-1],\n",
    "    hidden_size=HIDDEN_SIZE\n",
    ").to(DEVICE)\n",
    "\n",
    "optim = torch.optim.Adam(\n",
    "    params=model.parameters(),\n",
    "    **optim_kwargs\n",
    ")\n",
    "\n",
    "gauss_nll = torch.nn.GaussianNLLLoss(eps=1e-6, reduction=\"mean\") #torch.nn.MSELoss()\n",
    "\n",
    "def nll_criterion(pred_params, y):\n",
    "    mu, log_var = torch.chunk(pred_params, 2, dim=-1)\n",
    "    var = torch.nn.functional.softplus(log_var) + 1e-3\n",
    "    return gauss_nll(mu, y, var)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Scale KL term by dataset size; warm up to full weight over KL_WARMUP_EPOCHS.\n",
    "base_kl_weight =  1.0 / len(train_dataset)\n",
    "\n",
    "best_val = float(\"inf\")\n",
    "patience = 5\n",
    "wait = 0\n",
    "with mlflow.start_run(run_name=f\"baysian_lstm_nll_fs_06_load_calendar_future_weather\"):\n",
    "    mlflow.log_params(optim_kwargs)\n",
    "    mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
    "    mlflow.log_param(\"hidden_size\", HIDDEN_SIZE)\n",
    "    mlflow.log_param(\"sample_nbr\", SAMPLE_NBR)\n",
    "    mlflow.log_param(\"val_eval_sample_nbr\", VAL_EVAL_SAMPLE_NBR)\n",
    "    mlflow.log_param(\"kl_warmup_epochs\", KL_WARMUP_EPOCHS)\n",
    "    mlflow.log_param(\"base_kl_weight\", base_kl_weight)\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "        model.train()\n",
    "        train_elbo = 0.0\n",
    "\n",
    "        kl_weight = base_kl_weight * min(1.0, (epoch + 1) / KL_WARMUP_EPOCHS)\n",
    "\n",
    "        for xb, yb in train_dataloader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "            optim.zero_grad()\n",
    "\n",
    "            elbo = model.sample_elbo(\n",
    "                inputs=xb,\n",
    "                labels=yb,\n",
    "                criterion=nll_criterion,\n",
    "                sample_nbr=SAMPLE_NBR,\n",
    "                complexity_cost_weight=kl_weight\n",
    "            )\n",
    "\n",
    "            elbo.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optim.step()\n",
    "\n",
    "            train_elbo += elbo.item()\n",
    "\n",
    "        train_elbo /= len(train_dataloader)\n",
    "\n",
    "        mlflow.log_metric(\"train_elbo\", train_elbo, step=epoch)\n",
    "\n",
    "        # validate only 3 epochs\n",
    "        if epoch % 3 == 0:\n",
    "            model.eval()\n",
    "            val_elbo = 0.0\n",
    "            val_nll = 0.0\n",
    "\n",
    "            val_var_mean_acc = 0.0\n",
    "            val_var_min_acc = float(\"inf\")\n",
    "            val_var_max_acc = -float(\"inf\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for xb, yb in val_dataloader:\n",
    "                    xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "\n",
    "                    elbo = model.sample_elbo(\n",
    "                        inputs=xb,\n",
    "                        labels=yb,\n",
    "                        criterion=nll_criterion,\n",
    "                        sample_nbr=SAMPLE_NBR,\n",
    "                        complexity_cost_weight=kl_weight\n",
    "                    )\n",
    "\n",
    "                    pred_samples = torch.stack([model(xb) for _ in range(VAL_EVAL_SAMPLE_NBR)], dim=0)\n",
    "                    mu_s, log_var_s = torch.chunk(pred_samples, 2, dim=-1)\n",
    "                    var_s = torch.nn.functional.softplus(log_var_s) + 1e-6\n",
    "                    mu_pred = mu_s.mean(dim=0)\n",
    "                    var_pred = var_s.mean(dim=0) + mu_s.var(dim=0, unbiased=False)\n",
    "                    nll = gauss_nll(mu_pred, yb, var_pred)\n",
    "\n",
    "                    val_elbo += elbo.item()\n",
    "                    val_nll += nll.item()\n",
    "\n",
    "                    val_var_mean_acc += var_pred.mean().item()\n",
    "                    val_var_min_acc = min(val_var_min_acc, var_pred.min().item())\n",
    "                    val_var_max_acc = max(val_var_max_acc, var_pred.max().item())\n",
    "\n",
    "                val_elbo /= len(val_dataloader)\n",
    "                val_nll /= len(val_dataloader)\n",
    "                val_var_mean = val_var_mean_acc / len(val_dataloader)\n",
    "\n",
    "                mlflow.log_metric(\"val_elbo\", val_elbo, step=epoch)\n",
    "                mlflow.log_metric(\"val_nll\", val_nll, step=epoch)\n",
    "                mlflow.log_metric(\"kl_weight\", kl_weight, step=epoch)\n",
    "                \n",
    "                mlflow.log_metric(\"val_var_mean\", val_var_mean, step=epoch)\n",
    "                mlflow.log_metric(\"val_var_min\", val_var_min_acc, step=epoch)\n",
    "                mlflow.log_metric(\"val_var_max\", val_var_max_acc, step=epoch)\n",
    "\n",
    "                if val_nll < best_val:\n",
    "                    best_val = val_nll\n",
    "                    wait = 0\n",
    "                    best_state = copy.deepcopy(model.state_dict())\n",
    "                    best_epoch = epoch\n",
    "\n",
    "                else:\n",
    "                    wait +=1\n",
    "                \n",
    "                if wait >= patience:\n",
    "                    break\n",
    "\n",
    "                print(\n",
    "                    f\"Epoch {epoch+1}/{NUM_EPOCHS}, \"\n",
    "                    f\"Train ELBO: {train_elbo:.4f}, \"\n",
    "                    f\"Val ELBO: {val_elbo:.4f}, \"\n",
    "                    f\"Val NLL: {val_nll:.6f}, \"\n",
    "                    f\"KL w: {kl_weight:.2e}\"\n",
    "                )\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "        model.eval()\n",
    "        mlflow.log_metric(\"best_epoch\", best_epoch)\n",
    "\n",
    "        mlflow.log_dict(meta, \"artifacts/data_meta.json\")\n",
    "        mlflow.log_artifact(str(scalers_dir / \"y_scaler.pkl\"), artifact_path=\"preprocessing\")\n",
    "\n",
    "        mlflow.pytorch.log_model(\n",
    "            pytorch_model=model,\n",
    "            name=\"best_model\",\n",
    "            signature=signature,\n",
    "            input_example=input_example,\n",
    "            metadata={\n",
    "                \"feature_names\": meta[\"feature_names\"],\n",
    "                \"target\": \"actual_load_mw\",\n",
    "                \"scaler_type\": meta[\"scaler_type\"],\n",
    "                \"output_space\": \"scaled_target\",\n",
    "            },\n",
    "            tags={\"feature_set\": EXPERIMENT_NAME},\n",
    "        )\n",
    "\n",
    "        best_model = model\n",
    "\n",
    "        best_model.eval()\n",
    "\n",
    "        all_preds = []\n",
    "        all_trues = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in test_dataloader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                yb = yb.to(DEVICE)\n",
    "\n",
    "                samples = torch.stack([best_model(xb) for _ in range(TEST_SAMPLE_NBR)], dim=0)  # [S, N, 2H]\n",
    "                mu_s, log_var_s = torch.chunk(samples, 2, dim=-1)                                # [S, N, H], [S, N, H]\n",
    "                var_s = torch.nn.functional.softplus(log_var_s) + 1e-6\n",
    "\n",
    "                mu_scaled = mu_s.mean(dim=0)  # point forecast in scaled space, [N, H]\n",
    "                pred_var_scaled = var_s.mean(dim=0) + mu_s.var(dim=0, unbiased=False)  # total predictive var\n",
    "\n",
    "                N, H = mu_scaled.shape\n",
    "\n",
    "                y_pred = y_scaler.inverse_transform(mu_scaled.cpu().numpy().reshape(-1, 1)).reshape(N, H)\n",
    "                y_true = y_scaler.inverse_transform(yb.cpu().numpy().reshape(-1, 1)).reshape(N, H)\n",
    "\n",
    "                all_preds.append(y_pred)\n",
    "                all_trues.append(y_true)\n",
    "\n",
    "        # concat across batches\n",
    "        y_pred_full = np.concatenate(all_preds, axis=0)\n",
    "        y_true_full = np.concatenate(all_trues, axis=0)\n",
    "\n",
    "        mae = mean_absolute_error(y_true_full, y_pred_full)\n",
    "        rmse = root_mean_squared_error(y_true_full, y_pred_full)\n",
    "\n",
    "        mlflow.log_metric(\"test_mae_unscaled\", mae)\n",
    "        mlflow.log_metric(\"test_rmse_unscaled\", rmse)\n",
    "\n",
    "        for h in [0, 3, 11, 23, 47, 95]:\n",
    "            mlflow.log_metric(\n",
    "                f\"rmse_h{h+1}\",\n",
    "                root_mean_squared_error(y_true_full[:, h], y_pred_full[:, h])\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4e36b4",
   "metadata": {},
   "source": [
    "## Evaluation on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cef8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoggedModel(artifact_location='mlflow-artifacts:/3/models/m-59d7d88d721247bc91c1ced7635a401f/artifacts', creation_timestamp=1770719453301, experiment_id='3', last_updated_timestamp=1770719462357, model_id='m-59d7d88d721247bc91c1ced7635a401f', model_type='', model_uri='models:/m-59d7d88d721247bc91c1ced7635a401f', name='best_model', source_run_id='0df404bd6fee454788a60bacb548f643', status=<LoggedModelStatus.READY: 'READY'>, status_message='')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranked_checkpoints = mlflow.search_logged_models(\n",
    "    filter_string=f\"source_run_id='0df404bd6fee454788a60bacb548f643'\",\n",
    "    order_by=[{\"field_name\": \"metrics.val_nll\", \"ascending\": True}],\n",
    "    output_format=\"list\",\n",
    ")\n",
    "\n",
    "best_checkpoint = ranked_checkpoints[0]\n",
    "best_checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af8d76bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc67c94e4e934185b5bfd44ed1321c99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loaded_model = mlflow.pytorch.load_model(best_checkpoint.model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b5b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/05 12:26:31 INFO mlflow.system_metrics.system_metrics_monitor: Skip logging GPU metrics. Set logger level to DEBUG for more details.\n",
      "2026/02/05 12:26:31 INFO mlflow.system_metrics.system_metrics_monitor: Started monitoring system metrics.\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "loaded_model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_trues = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dataloader:\n",
    "        xb = xb.to(DEVICE)\n",
    "        yb = yb.to(DEVICE)\n",
    "\n",
    "        samples = torch.stack([loaded_model(xb) for _ in range(TEST_SAMPLE_NBR)], dim=0)  # [S, N, 2H]\n",
    "        mu_s, log_var_s = torch.chunk(samples, 2, dim=-1)                                # [S, N, H], [S, N, H]\n",
    "        var_s = torch.nn.functional.softplus(log_var_s) + 1e-6\n",
    "\n",
    "        mu_scaled = mu_s.mean(dim=0)  # point forecast in scaled space, [N, H]\n",
    "        pred_var_scaled = var_s.mean(dim=0) + mu_s.var(dim=0, unbiased=False)  # total predictive var\n",
    "\n",
    "        N, H = mu_scaled.shape\n",
    "\n",
    "        y_pred = y_scaler.inverse_transform(mu_scaled.cpu().numpy().reshape(-1, 1)).reshape(N, H)\n",
    "        y_true = y_scaler.inverse_transform(yb.cpu().numpy().reshape(-1, 1)).reshape(N, H)\n",
    "\n",
    "        all_preds.append(y_pred)\n",
    "        all_trues.append(y_true)\n",
    "\n",
    "# concat across batches\n",
    "y_pred_full = np.concatenate(all_preds, axis=0)\n",
    "y_true_full = np.concatenate(all_trues, axis=0)\n",
    "\n",
    "mae = mean_absolute_error(y_true_full, y_pred_full)\n",
    "rmse = root_mean_squared_error(y_true_full, y_pred_full)\n",
    "\n",
    "mlflow.log_metric(\"test_mae_unscaled\", mae)\n",
    "mlflow.log_metric(\"test_rmse_unscaled\", rmse)\n",
    "\n",
    "for h in [0, 3, 11, 23, 47, 95]:\n",
    "    mlflow.log_metric(\n",
    "        f\"rmse_h{h+1}\",\n",
    "        root_mean_squared_error(y_true_full[:, h], y_pred_full[:, h])\n",
    "    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
